"""
Predictions endpoint - ML integration with Hugging Face API
Provides access to all 3 ML models for flight predictions
"""
from fastapi import APIRouter, HTTPException, Depends
from sqlalchemy.ext.asyncio import AsyncSession
from typing import Dict

from app.schemas.prediction import (
    FlightPredictionRequest,
    FlightPredictionResponse,
    MLHealthResponse,
    MLModelsInfoResponse
)
from app.services.external.ml_client import MLAPIClient
from app.api.v1.endpoints.auth import get_current_active_user
from app.models.user import User
from app.database import get_db
from app.core.logging import logger

router = APIRouter()


@router.post(
    "/predict",
    response_model=FlightPredictionResponse,
    summary="Get ML prediction for a flight",
    description="""
    Performs a complete ML prediction using all 3 models:
    - Model 1: ETA/ETD prediction with delay probabilities
    - Model 2: Parking occupation duration prediction
    - Model 3: Conflict detection and decision recommendation
    
    The prediction is performed by the Hugging Face Space API:
    https://tagba-ubuntuairlab.hf.space
    """
)
async def predict_flight(
    request: FlightPredictionRequest,
    current_user: User = Depends(get_current_active_user),
    db: AsyncSession = Depends(get_db)
) -> FlightPredictionResponse:
    """
    Predict flight metrics using ML models with real-time data enrichment
    
    Args:
        request: Flight data for prediction
        current_user: Authenticated user
        db: Database session
    
    Returns:
        Complete prediction from all 3 models
    
    Raises:
        HTTPException: If ML API is unavailable or prediction fails
    """
    try:
        # Enrichir les donnÃ©es de requÃªte avec des statistiques rÃ©elles de la DB
        from app.services.business.traffic_stats_service import (
            get_traffic_statistics,
            get_weather_data,
            get_historical_data
        )
        from app.repositories.flight_repository import FlightRepository
        
        # RÃ©cupÃ©rer les statistiques de trafic en temps rÃ©el (avec gestion d'erreur)
        try:
            traffic_stats = await get_traffic_statistics(db)
        except Exception as e:
            logger.warning(f"Error getting traffic stats, using defaults: {str(e)}")
            await db.rollback()  # Reset transaction
            traffic_stats = {
                "approaching": 5, "tarmac_occupation": 0.65, "available_spots": 12,
                "current_occupation": 0.70, "incoming": 8, "outgoing": 6, "future_free_spots": 3
            }
        
        # RÃ©cupÃ©rer les donnÃ©es mÃ©tÃ©o (pour l'instant simulÃ©es)
        weather = await get_weather_data()
        
        # Si un icao24 est fourni, rÃ©cupÃ©rer le vol de la DB pour donnÃ©es historiques
        historical_data = {}
        if request.icao24:
            try:
                flight_repo = FlightRepository(db)
                flight = await flight_repo.get_by_icao24(request.icao24)
                if flight:
                    historical_data = await get_historical_data(db, flight)
            except Exception as e:
                logger.warning(f"Error getting historical data for {request.icao24}, using defaults: {str(e)}")
                await db.rollback()  # Reset transaction
                historical_data = {}
        
        # FORCER le remplacement par les donnÃ©es rÃ©elles de la DB (pas de donnÃ©es fictives)
        enriched_data = request.model_dump()
        
        # Remplacer TOUTES les statistiques de trafic par les vraies donnÃ©es temps rÃ©el
        enriched_data["trafic_approche"] = traffic_stats["approaching"]
        enriched_data["occupation_tarmac"] = traffic_stats["tarmac_occupation"]
        enriched_data["disponibilite_emplacements"] = traffic_stats["available_spots"]
        enriched_data["occupation_actuelle"] = traffic_stats["current_occupation"]
        enriched_data["trafic_entrant"] = traffic_stats["incoming"]
        enriched_data["trafic_sortant"] = traffic_stats["outgoing"]
        enriched_data["emplacements_futurs_libres"] = traffic_stats["future_free_spots"]
        
        # Remplacer les donnÃ©es mÃ©tÃ©o par les vraies donnÃ©es
        enriched_data["temperature"] = weather["temperature"]
        enriched_data["vent_vitesse"] = weather["wind_speed"]
        enriched_data["visibilite"] = weather["visibility"]
        enriched_data["pluie"] = weather["rain"]
        enriched_data["meteo_score"] = weather["score"]
        
        # Remplacer les donnÃ©es historiques si disponibles depuis la DB
        if historical_data:
            enriched_data["retard_historique_compagnie"] = historical_data.get("avg_delay", 5.0)
            enriched_data["type_avion"] = historical_data.get("aircraft_type", enriched_data.get("type_avion", "A320"))
            enriched_data["historique_occupation_avion"] = historical_data.get("avg_occupation_time", 45.0)
            enriched_data["passagers_estimes"] = historical_data.get("passengers", 180)
            enriched_data["compagnie"] = historical_data.get("airline", enriched_data.get("compagnie", "Unknown"))
        else:
            # Valeurs par dÃ©faut si pas de donnÃ©es historiques
            enriched_data["retard_historique_compagnie"] = enriched_data.get("retard_historique_compagnie") or 5.0
            enriched_data["type_avion"] = enriched_data.get("type_avion") or "A320"
            enriched_data["historique_occupation_avion"] = enriched_data.get("historique_occupation_avion") or 45.0
            enriched_data["passagers_estimes"] = enriched_data.get("passagers_estimes") or 180
            enriched_data["compagnie"] = enriched_data.get("compagnie") or "Unknown"
        
        # Assurer que priorite_vol est dÃ©fini
        enriched_data["priorite_vol"] = enriched_data.get("priorite_vol", 0)
        
        # Log des donnÃ©es rÃ©elles utilisÃ©es pour la prÃ©diction
        logger.info(
            f"ðŸ”„ PrÃ©diction ML avec DONNÃ‰ES RÃ‰ELLES (pas de mock) - Flight: {request.callsign or 'unknown'}\n"
            f"  ðŸ“Š Trafic DB: approaching={traffic_stats['approaching']}, "
            f"occupation={traffic_stats['current_occupation']*100:.0f}%, "
            f"spots_dispos={traffic_stats['available_spots']}\n"
            f"  ðŸŒ¤ï¸  MÃ©tÃ©o: temp={weather['temperature']}Â°C, vent={weather['wind_speed']}km/h, "
            f"visibilitÃ©={weather['visibility']}km\n"
            f"  âœˆï¸  Historique: delay_avg={enriched_data['retard_historique_compagnie']}min, "
            f"occupation_avg={enriched_data['historique_occupation_avion']}min"
        )
        
        async with MLAPIClient() as ml_client:
            # Call Hugging Face ML API avec donnÃ©es enrichies
            prediction = await ml_client.predict(enriched_data)
            
            # CrÃ©er la rÃ©ponse avant tout logging pour Ã©viter les problÃ¨mes greenlet
            response = FlightPredictionResponse(**prediction)
            
            logger.info(
                f"ML prediction successful for flight {request.callsign or 'unknown'}"
            )
            
            return response
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Unexpected error in ML prediction: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Erreur lors de la prÃ©diction ML: {str(e)}"
        )


@router.get(
    "/health",
    response_model=MLHealthResponse,
    summary="Check ML API health",
    description="Verifies that the ML API and models are operational"
)
async def check_ml_health(
    current_user: User = Depends(get_current_active_user)
) -> MLHealthResponse:
    """
    Check health status of ML API
    
    Args:
        current_user: Authenticated user
    
    Returns:
        Health status including models availability
    
    Raises:
        HTTPException: If ML API is unreachable
    """
    try:
        async with MLAPIClient() as ml_client:
            health = await ml_client.health_check()
            
            logger.info(
                f"ML health check: {health['status']} "
                f"(user: {current_user.username})"
            )
            
            return MLHealthResponse(**health)
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ML health check failed: {str(e)}")
        raise HTTPException(
            status_code=503,
            detail=f"ML API indisponible: {str(e)}"
        )


@router.get(
    "/models/info",
    response_model=MLModelsInfoResponse,
    summary="Get ML models information",
    description="Returns detailed information about the 3 ML models"
)
async def get_models_info(
    current_user: User = Depends(get_current_active_user)
) -> MLModelsInfoResponse:
    """
    Get information about ML models
    
    Args:
        current_user: Authenticated user
    
    Returns:
        Models details (type, features, outputs)
    
    Raises:
        HTTPException: If ML API is unreachable
    """
    try:
        async with MLAPIClient() as ml_client:
            info = await ml_client.get_models_info()
            
            logger.info(
                f"ML models info retrieved (user: {current_user.username})"
            )
            
            return MLModelsInfoResponse(**info)
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get models info: {str(e)}")
        raise HTTPException(
            status_code=503,
            detail=f"Impossible de rÃ©cupÃ©rer les infos ML: {str(e)}"
        )


@router.post(
    "/predict/batch",
    response_model=Dict,
    summary="Batch prediction for multiple flights",
    description="Performs predictions for multiple flights in parallel"
)
async def predict_batch(
    requests: list[FlightPredictionRequest],
    current_user: User = Depends(get_current_active_user),
    db: AsyncSession = Depends(get_db)
) -> Dict:
    """
    Batch prediction for multiple flights
    
    Args:
        requests: List of flight data
        current_user: Authenticated user
        db: Database session
    
    Returns:
        Dictionary with results and errors
    
    Note:
        This endpoint processes predictions sequentially to avoid
        overwhelming the ML API. For production, consider implementing
        a queue system.
    """
    if len(requests) > 50:
        raise HTTPException(
            status_code=400,
            detail="Maximum 50 flights per batch request"
        )
    
    results = []
    errors = []
    
    async with MLAPIClient() as ml_client:
        for idx, request in enumerate(requests):
            try:
                prediction = await ml_client.predict(request.model_dump())
                results.append({
                    "index": idx,
                    "callsign": request.callsign,
                    "prediction": prediction
                })
            except Exception as e:
                errors.append({
                    "index": idx,
                    "callsign": request.callsign,
                    "error": str(e)
                })
    
    logger.info(
        f"Batch prediction: {len(results)} success, {len(errors)} errors "
        f"(user: {current_user.username})"
    )
    
    return {
        "total": len(requests),
        "success": len(results),
        "errors": len(errors),
        "results": results,
        "failed": errors
    }
